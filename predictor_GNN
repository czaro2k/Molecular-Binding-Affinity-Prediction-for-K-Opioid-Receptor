!pip install torch-geometric
!pip install rdkit-pypi

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import itertools
from torch_geometric.data import Data, DataLoader, Batch
from torch_geometric.nn import GCNConv, global_mean_pool
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors, PandasTools
from rdkit.Chem.EState import Fingerprinter as EStateFingerprinter
from rdkit.Avalon import pyAvalonTools
from torch.utils.data import TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, matthews_corrcoef
from scipy.stats import pearsonr



# Define the atom_features and bond_features functions
def atom_features(atom):
    """Returns a list of atom features."""
    return [
        atom.GetAtomicNum(),                  # Atomic number
        atom.GetDegree(),                     # Degree of the atom
        atom.GetImplicitValence(),            # Implicit valence of the atom
        atom.GetFormalCharge(),               # Formal charge of the atom
        atom.GetNumRadicalElectrons(),        # Number of radical electrons
        int(atom.GetIsAromatic())             # Aromaticity as int (0 or 1)
    ]

def bond_features(bond):
    """Returns a list of bond features."""
    return [
        bond.GetBondTypeAsDouble(),           # Bond type as a double value
        int(bond.GetIsConjugated()),          # Conjugation as int (0 or 1)
        int(bond.IsInRing())                  # Whether the bond is in a ring as int (0 or 1)
    ]

# SMILES to Graph function
def smiles_to_graph(smiles):
    # Convert SMILES string to an RDKit molecule object
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        raise ValueError("Invalid SMILES string")

    # Add hydrogens to the molecule
    mol = Chem.AddHs(mol)

    # Compute atom features
    atom_features_list = [atom_features(atom) for atom in mol.GetAtoms()]
    x = torch.tensor(atom_features_list, dtype=torch.float)

    # Compute edge index and edge features
    edge_index = []
    edge_attr = []

    for bond in mol.GetBonds():
        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
        edge_index.append([start, end])
        edge_index.append([end, start])  # Include both directions for undirected graph
        edge_attr.append(bond_features(bond))
        edge_attr.append(bond_features(bond))  # Same features for both directions

    # Convert to torch tensors
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # shape [2, num_edges]
    edge_attr = torch.tensor(edge_attr, dtype=torch.float)

    # Create a PyTorch Geometric data object
    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)

    return data

# Load CSV file with pandas
csv_file = "data_clean_kop.csv"
df = pd.read_csv(csv_file)

# Assuming the CSV has a column named 'SMILES' containing SMILES strings
# Create empty lists to store features
atom_features_list = []
bond_features_list = []
edge_index_list = []
smiles_list = []

# Iterate through each SMILES string in the CSV
for smiles in df['SMILES']:  # Replace 'SMILES' with the actual column name in your CSV
    try:
        # Convert SMILES to graph data
        graph_data = smiles_to_graph(smiles)

        # Append extracted features to lists
        atom_features_list.append(graph_data.x.tolist())  # Node/atom features
        bond_features_list.append(graph_data.edge_attr.tolist())  # Bond/edge features
        edge_index_list.append(graph_data.edge_index.tolist())  # Edge indices
        smiles_list.append(smiles)  # Store the SMILES for reference

    except ValueError as e:
        print(f"Error processing SMILES {smiles}: {e}")

# Add the new features (atom_features, bond_features, edge_index) as new columns to the existing DataFrame
df['atom_features'] = atom_features_list
df['bond_features'] = bond_features_list
df['edge_index'] = edge_index_list

# Display the updated DataFrame with both original columns and the newly added graph features

df.head()



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data
import pandas as pd

class GNNVectorizer(nn.Module):
    def __init__(self, input_dim, hidden_dim, vector_dim, dropout_prob=0.5):
        super(GNNVectorizer, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)
        self.fc1 = nn.Linear(hidden_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, vector_dim)  # Vector representation
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, data):
        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = F.relu(self.conv2(x, edge_index))
        x = self.dropout(x)
        x = F.relu(self.conv3(x, edge_index))
        x = self.dropout(x)
        pooled = global_mean_pool(x, batch)
        x = F.relu(self.fc1(pooled))
        vector_output = self.fc2(x)  # Only vector representation
        return vector_output

# Function to create Data objects from the DataFrame
def create_graph_data(row):
    atom_features = torch.tensor(row['atom_features'], dtype=torch.float)
    bond_features = torch.tensor(row['bond_features'], dtype=torch.float)
    edge_index = torch.tensor(row['edge_index'], dtype=torch.long)
    data = Data(x=atom_features, edge_index=edge_index, edge_attr=bond_features)
    return data

# Creating a DataLoader from the DataFrame
def create_dataloader(df, batch_size=32):
    data_list = [create_graph_data(row) for _, row in df.iterrows()]
    loader = DataLoader(data_list, batch_size=batch_size, shuffle=False)
    return loader

# Model initialization
input_dim = len(df.iloc[0]['atom_features'][0])  # Number of atom features per node
hidden_dim = 64
vector_dim = 128  # Vector representation dimension

# Instantiate the model
model = GNNVectorizer(input_dim, hidden_dim, vector_dim)

# Create DataLoader using your DataFrame
data_loader = create_dataloader(df)

# Prepare lists to collect results
results = {
    "SMILES": [],
    "pCHEMBL": [],
    "vector_representation": []
}

# Store SMILES and pCHEMBL for each graph
smiles_list = df['SMILES'].tolist()
pchembl_list = df['pCHEMBL'].tolist()

# Forward pass through the model and collect results
model.eval()
with torch.no_grad():
    for batch in data_loader:
        # Move batch to GPU if needed
        vector_representation = model(batch)
        batch_size = batch.num_graphs
        for i in range(batch_size):
            # Collect results using the indices from the batch
            index = batch.batch == i  # Find the indices of the current batch graph
            results["SMILES"].append(smiles_list[i])
            results["pCHEMBL"].append(pchembl_list[i])
            results["vector_representation"].append(vector_representation[i].cpu().numpy())

# Create a DataFrame with SMILES and the vector representation
results_df = df[['SMILES', 'pCHEMBL']].copy()
results_df['vector_representation'] = results["vector_representation"]

results_df.head()

class GCNModel(torch.nn.Module):
    def __init__(self, atom_feature_dim, bond_feature_dim, hidden_dim, output_dim):
        super(GCNModel, self).__init__()
        # Initialize GCN layers
        self.conv1 = GCNConv(atom_feature_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)

        # Fully connected layer to produce the output vector
        self.fc = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        # Message passing through 3 GCN layers
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()

        # Global mean pooling
        x = global_mean_pool(x, data.batch)

        # Fully connected layer
        x = self.fc(x)

        return x

# Function to plot learning curves
def plot_learning_curves(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Learning Curves')
    plt.show()

# Function to plot predicted vs actual values
def plot_predictions(y_true, y_pred):
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, y_true, alpha=0.5)
    plt.xlabel('Predicted Values')
    plt.ylabel('Actual Values')

    x_diag = np.linspace(min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max()), num=10000)
    plt.plot(x_diag, x_diag, color='red')
    plt.title('Predicted vs Actual Values')
    plt.show()

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    pearson_corr, _ = pearsonr(y_true, y_pred)
    ccc = 2 * pearson_corr / (np.var(y_true) + np.var(y_pred) + (np.mean(y_true) - np.mean(y_pred))**2)
    mcc = matthews_corrcoef(np.round(y_true), np.round(y_pred))

    print(f'MSE: {mse:.4f}')
    print(f'CCC: {ccc:.4f}')
    print(f'MCC: {mcc:.4f}')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

def train_model(result_df, model_class, epochs=250, batch_size=32):
    # Extract features and labels from DataFrame
    X = np.stack(result_df['vector_representation'].values)  # Use vector representation
    y = result_df['pCHEMBL'].values  # Labels

    # Split the data into training, validation, and test sets
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # Standardize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    # Convert to PyTorch tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

    # Create DataLoader for training and validation
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Initialize the model, loss function, and optimizer
    input_dim = X_train.shape[1]
    model = model_class(input_dim)
    criterion = nn.MSELoss()  # Use MSELoss for regression
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Training loop with early stopping
    patience = 20
    best_val_loss = float('inf')
    no_improve_epochs = 0
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        model.train()
        epoch_train_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            epoch_train_loss += loss.item() * batch_X.size(0)  # Accumulate loss

        epoch_train_loss /= len(train_loader.dataset)  # Average loss over all batches
        train_losses.append(epoch_train_loss)

        # Validate the model
        model.eval()
        epoch_val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                val_outputs = model(batch_X)
                val_loss = criterion(val_outputs, batch_y)
                epoch_val_loss += val_loss.item() * batch_X.size(0)  # Accumulate loss

        epoch_val_loss /= len(val_loader.dataset)  # Average loss over all batches
        val_losses.append(epoch_val_loss)

        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            no_improve_epochs = 0
            torch.save(model.state_dict(), 'best_model.pth')  # Save the model if it's the best
        else:
            no_improve_epochs += 1

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')

        if no_improve_epochs >= patience:
            print('Early stopping triggered')
            break

    # Plot learning curves
    plot_learning_curves(train_losses, val_losses)

    # Load the best model for evaluation
    model.load_state_dict(torch.load('best_model.pth'))

    # Evaluate on test set
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        test_loss = criterion(test_outputs, y_test_tensor)
        print(f'Test Loss: {test_loss.item():.4f}')

        # Plot predictions
        y_pred = test_outputs.numpy().flatten()
        y_true = y_test_tensor.numpy().flatten()
        plot_predictions(y_true, y_pred)

        # Calculate metrics
        calculate_metrics(y_true, y_pred)

# Define the neural network model
class SimpleNN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)  # Output layer for regression
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)  # No activation function here
        return x

class DeepNN(nn.Module):
    def __init__(self, input_dim):
        super(DeepNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 8192)
        self.fc2 = nn.Linear(8192, 4096)
        self.fc3 = nn.Linear(4096, 2048)
        self.fc4 = nn.Linear(2048, 512)
        self.fc5 = nn.Linear(512, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)  # Adjusted dropout rate
        self.batchnorm1 = nn.BatchNorm1d(8192)
        self.batchnorm2 = nn.BatchNorm1d(4096)
        self.batchnorm3 = nn.BatchNorm1d(2048)
        self.batchnorm4 = nn.BatchNorm1d(512)

    def forward(self, x):
        x = self.fc1(x)
        x = self.batchnorm1(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc2(x)
        x = self.batchnorm2(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc3(x)
        x = self.batchnorm3(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.fc4(x)
        x = self.batchnorm4(x)
        x = self.relu(x)

        x = self.fc5(x)  # Output layer for regression
        return x


train_model(results_df, SimpleNN, epochs = 200)
